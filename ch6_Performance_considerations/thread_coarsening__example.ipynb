{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iakEJhkgzLJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a0693c1-e218-4b31-aa3c-238c9dc1de94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jan 25 14:12:04 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tiledMatrixMult.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "#include <time.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define TILE_WIDTH 32\n",
        "\n",
        "inline cudaError_t checkCuda(cudaError_t result) {\n",
        "    if (result != cudaSuccess) {\n",
        "        fprintf(stderr, \"Cuda Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
        "        assert(result == cudaSuccess);\n",
        "    }\n",
        "    return result;\n",
        "}\n",
        "\n",
        "__global__ void tiledMatrixMultKernel(float *M, float *N, float *P, int width) {\n",
        "    __shared__ float M_s[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ float N_s[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    int bx = blockIdx.x;   int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;  int ty = threadIdx.y;\n",
        "\n",
        "    // Identify the row and column of the P element to work on\n",
        "    int row = by * TILE_WIDTH + ty;\n",
        "    int col = bx * TILE_WIDTH + tx;\n",
        "\n",
        "    // Loop over the M and N tiles required to compute P element\n",
        "    float Pvalue = 0;\n",
        "    for (int tile = 0; tile < (width + TILE_WIDTH - 1) / TILE_WIDTH; tile++) {\n",
        "\n",
        "        // Collaborative loading of M and N tiles into shared memory\n",
        "        if (row < width && (tile * TILE_WIDTH + tx) < width) {\n",
        "            M_s[ty][tx] = M[row * width + (tile * TILE_WIDTH + tx)];\n",
        "        } else {\n",
        "            M_s[ty][tx] = 0.0f;  // Zero-padding for out-of-bounds accesses\n",
        "        }\n",
        "\n",
        "        if ((tile * TILE_WIDTH + ty) < width && col < width) {\n",
        "            N_s[ty][tx] = N[(tile * TILE_WIDTH + ty) * width + col];\n",
        "        } else {\n",
        "            N_s[ty][tx] = 0.0f;  // Zero-padding for out-of-bounds accesses\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        // Compute the partial product for the tile\n",
        "        for (int k = 0; k < TILE_WIDTH; k++) {\n",
        "            Pvalue += M_s[ty][k] * N_s[k][tx];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write the result to the output matrix P\n",
        "    if (row < width && col < width) {\n",
        "        P[row * width + col] = Pvalue;\n",
        "    }\n",
        "}\n",
        "\n",
        "void tiledMatrixMult(float *M_h, float *N_h, float *P_h, int width) {\n",
        "    // Allocate GPU memory\n",
        "    float *M_d, *N_d, *P_d;\n",
        "    checkCuda( cudaMalloc((void**)&M_d, width * width * sizeof(float)) );\n",
        "    checkCuda( cudaMalloc((void**)&N_d, width * width * sizeof(float)) );\n",
        "    checkCuda( cudaMalloc((void**)&P_d, width * width * sizeof(float)) );\n",
        "\n",
        "    // Transfer data host -> device\n",
        "    checkCuda( cudaMemcpy(M_d, M_h, width * width * sizeof(float), cudaMemcpyHostToDevice) );\n",
        "    checkCuda( cudaMemcpy(N_d, N_h, width * width * sizeof(float), cudaMemcpyHostToDevice) );\n",
        "\n",
        "    // Perform mmult\n",
        "    dim3 numThreadsPerBlock(32, 32);\n",
        "    dim3 numBlocks((width + numThreadsPerBlock.x - 1) / numThreadsPerBlock.x,\n",
        "                   (width + numThreadsPerBlock.y - 1) / numThreadsPerBlock.y\n",
        "    );\n",
        "    tiledMatrixMultKernel<<< numBlocks, numThreadsPerBlock >>>(M_d, N_d, P_d, width);\n",
        "    checkCuda( cudaGetLastError() );\n",
        "    checkCuda( cudaDeviceSynchronize() );\n",
        "\n",
        "    // Transfer data device -> host\n",
        "    checkCuda( cudaMemcpy(P_h, P_d, width * width * sizeof(float), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "    // Free GPU memory\n",
        "    checkCuda( cudaFree(M_d) );\n",
        "    checkCuda( cudaFree(N_d) );\n",
        "    checkCuda( cudaFree(P_d) );\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "    srand(time(NULL));\n",
        "\n",
        "    // N x N matrix\n",
        "    int n = 1 << 12;\n",
        "\n",
        "    float *M = (float*) malloc(n * n * sizeof(float));\n",
        "    float *N = (float*) malloc(n * n * sizeof(float));\n",
        "    float *P = (float*) malloc(n * n * sizeof(float));\n",
        "\n",
        "    if(!M || !N || !P) {\n",
        "        fprintf(stderr, \"Malloc Error.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < n * n; i++) {\n",
        "        M[i] = rand() / (float)RAND_MAX;\n",
        "        N[i] = rand() / (float)RAND_MAX;\n",
        "    }\n",
        "\n",
        "    tiledMatrixMult(M, N, P, n);\n",
        "\n",
        "    printf(\"P[0] = %f | Expected:\", P[0]);\n",
        "    float sum = 0;\n",
        "    for (int k = 0; k < n; k++) {\n",
        "        sum += M[k] * N[k * n];\n",
        "    }\n",
        "    printf(\"%f\\n\", sum);\n",
        "\n",
        "\n",
        "    free(M);\n",
        "    free(N);\n",
        "    free(P);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "3eKOEKtH2y5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dafde1c6-e4da-4520-9c48-33769561dec7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tiledMatrixMult.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc tiledMatrixMult.cu -o tiledMatrixMult\n",
        "nvprof ./tiledMatrixMult"
      ],
      "metadata": {
        "id": "rSTOjG2O22I0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9525e41-2883-4b31-f9e2-c46d8a6237e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1284== NVPROF is profiling process 1284, command: ./tiledMatrixMult\n",
            "P[0] = 1011.952698 | Expected:1011.952698\n",
            "==1284== Profiling application: ./tiledMatrixMult\n",
            "==1284== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   77.21%  249.63ms         1  249.63ms  249.63ms  249.63ms  tiledMatrixMultKernel(float*, float*, float*, int)\n",
            "                   14.08%  45.533ms         1  45.533ms  45.533ms  45.533ms  [CUDA memcpy DtoH]\n",
            "                    8.71%  28.167ms         2  14.083ms  13.987ms  14.179ms  [CUDA memcpy HtoD]\n",
            "      API calls:   58.84%  249.64ms         1  249.64ms  249.64ms  249.64ms  cudaDeviceSynchronize\n",
            "                   22.46%  95.294ms         3  31.765ms  144.60us  94.994ms  cudaMalloc\n",
            "                   17.79%  75.491ms         3  25.164ms  14.209ms  46.924ms  cudaMemcpy\n",
            "                    0.82%  3.4651ms         3  1.1550ms  197.05us  2.1414ms  cudaFree\n",
            "                    0.05%  220.27us         1  220.27us  220.27us  220.27us  cudaLaunchKernel\n",
            "                    0.03%  131.34us       114  1.1520us     147ns  51.891us  cuDeviceGetAttribute\n",
            "                    0.00%  11.796us         1  11.796us  11.796us  11.796us  cuDeviceGetName\n",
            "                    0.00%  5.3990us         1  5.3990us  5.3990us  5.3990us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.0390us         1  4.0390us  4.0390us  4.0390us  cuDeviceTotalMem\n",
            "                    0.00%  1.2180us         3     406ns     162ns     818ns  cuDeviceGetCount\n",
            "                    0.00%  1.0690us         2     534ns     181ns     888ns  cuDeviceGet\n",
            "                    0.00%     570ns         1     570ns     570ns     570ns  cudaGetLastError\n",
            "                    0.00%     515ns         1     515ns     515ns     515ns  cuModuleGetLoadingMode\n",
            "                    0.00%     239ns         1     239ns     239ns     239ns  cuDeviceGetUuid\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile coarsedTileMatrixMultKernel.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "#include <time.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define TILE_WIDTH 32\n",
        "#define COARSE_FACTOR 4\n",
        "\n",
        "inline cudaError_t checkCuda(cudaError_t result) {\n",
        "    if (result != cudaSuccess) {\n",
        "        fprintf(stderr, \"Cuda Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
        "        assert(result == cudaSuccess);\n",
        "    }\n",
        "    return result;\n",
        "}\n",
        "\n",
        "__global__ void coarsedTiledMatrixMultKernel(float *M, float *N, float *P, int width) {\n",
        "    __shared__ float M_s[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ float N_s[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    int bx = blockIdx.x;   int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;  int ty = threadIdx.y;\n",
        "\n",
        "    // Identify the row and column of the P element to work on\n",
        "    int row = by * TILE_WIDTH + ty;\n",
        "    int colStart = bx * TILE_WIDTH * COARSE_FACTOR + tx;\n",
        "\n",
        "    // Initialize Pvalue for all output elements\n",
        "    float Pvalue[COARSE_FACTOR];\n",
        "    for (int c = 0; c < COARSE_FACTOR; c++) {\n",
        "        Pvalue[c] = 0.0f;\n",
        "    }\n",
        "\n",
        "    // Loop over the M and N tiles required to compute P element\n",
        "    for (int tile = 0; tile < width/TILE_WIDTH; tile++) {\n",
        "\n",
        "        // Collaborative loading of M tile into shared memory\n",
        "        M_s[ty][tx] = M[row * width + tile*TILE_WIDTH + tx];\n",
        "\n",
        "        for (int c = 0; c < COARSE_FACTOR; c++) {\n",
        "            int col = colStart + c * TILE_WIDTH;\n",
        "\n",
        "            // Collaborative loading of N tile into shared memory\n",
        "            N_s[ty][tx] = N[(tile*TILE_WIDTH + ty) * width + col];\n",
        "            __syncthreads();\n",
        "\n",
        "            for (int k = 0; k < TILE_WIDTH; k++) {\n",
        "                Pvalue[c] += M_s[ty][k] * N_s[k][tx];\n",
        "            }\n",
        "            __syncthreads();\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for (int c = 0; c < COARSE_FACTOR; c++) {\n",
        "        int col = colStart + c * TILE_WIDTH;\n",
        "        P[row * width + col] = Pvalue[c];\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "void tiledMatrixMult(float *M_h, float *N_h, float *P_h, int width) {\n",
        "    // Allocate GPU memory\n",
        "    float *M_d, *N_d, *P_d;\n",
        "    checkCuda( cudaMalloc((void**)&M_d, width * width * sizeof(float)) );\n",
        "    checkCuda( cudaMalloc((void**)&N_d, width * width * sizeof(float)) );\n",
        "    checkCuda( cudaMalloc((void**)&P_d, width * width * sizeof(float)) );\n",
        "\n",
        "    // Transfer data host -> device\n",
        "    checkCuda( cudaMemcpy(M_d, M_h, width * width * sizeof(float), cudaMemcpyHostToDevice) );\n",
        "    checkCuda( cudaMemcpy(N_d, N_h, width * width * sizeof(float), cudaMemcpyHostToDevice) );\n",
        "\n",
        "    // Perform mmult\n",
        "    dim3 numThreadsPerBlock(32, 32);\n",
        "    dim3 numBlocks((width + numThreadsPerBlock.x * COARSE_FACTOR - 1) / (numThreadsPerBlock.x * COARSE_FACTOR),\n",
        "                   (width + numThreadsPerBlock.y - 1) / numThreadsPerBlock.y\n",
        "    );\n",
        "    coarsedTiledMatrixMultKernel<<< numBlocks, numThreadsPerBlock >>>(M_d, N_d, P_d, width);\n",
        "    checkCuda( cudaGetLastError() );\n",
        "    checkCuda( cudaDeviceSynchronize() );\n",
        "\n",
        "    // Transfer data device -> host\n",
        "    checkCuda( cudaMemcpy(P_h, P_d, width * width * sizeof(float), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "    // Free GPU memory\n",
        "    checkCuda( cudaFree(M_d) );\n",
        "    checkCuda( cudaFree(N_d) );\n",
        "    checkCuda( cudaFree(P_d) );\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "    srand(time(NULL));\n",
        "\n",
        "    // N x N matrix\n",
        "    int n = 1 << 12;\n",
        "\n",
        "    float *M = (float*) malloc(n * n * sizeof(float));\n",
        "    float *N = (float*) malloc(n * n * sizeof(float));\n",
        "    float *P = (float*) malloc(n * n * sizeof(float));\n",
        "\n",
        "    if(!M || !N || !P) {\n",
        "        fprintf(stderr, \"Malloc Error.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < n * n; i++) {\n",
        "        M[i] = rand() / (float)RAND_MAX;\n",
        "        N[i] = rand() / (float)RAND_MAX;\n",
        "    }\n",
        "\n",
        "    tiledMatrixMult(M, N, P, n);\n",
        "\n",
        "    printf(\"P[0] = %f | Expected:\", P[0]);\n",
        "    float sum = 0;\n",
        "    for (int k = 0; k < n; k++) {\n",
        "        sum += M[k] * N[k * n];\n",
        "    }\n",
        "    printf(\"%f\\n\", sum);\n",
        "\n",
        "\n",
        "    free(M);\n",
        "    free(N);\n",
        "    free(P);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q95hf78RWh-x",
        "outputId": "484ae45f-8981-4c51-b715-19042295bb55"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting coarsedTileMatrixMultKernel.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc coarsedTileMatrixMultKernel.cu -o coarsedTileMatrixMultKernel\n",
        "nvprof ./coarsedTileMatrixMultKernel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z915zgDLWo_O",
        "outputId": "0df8aa5e-28b3-4fae-944c-91d87b53c148"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1412== NVPROF is profiling process 1412, command: ./coarsedTileMatrixMultKernel\n",
            "P[0] = 1012.294006 | Expected:1012.294006\n",
            "==1412== Profiling application: ./coarsedTileMatrixMultKernel\n",
            "==1412== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   74.38%  245.73ms         1  245.73ms  245.73ms  245.73ms  coarsedTiledMatrixMultKernel(float*, float*, float*, int)\n",
            "                   16.61%  54.862ms         1  54.862ms  54.862ms  54.862ms  [CUDA memcpy DtoH]\n",
            "                    9.01%  29.778ms         2  14.889ms  14.563ms  15.216ms  [CUDA memcpy HtoD]\n",
            "      API calls:   55.03%  245.73ms         1  245.73ms  245.73ms  245.73ms  cudaDeviceSynchronize\n",
            "                   24.81%  110.81ms         3  36.938ms  205.42us  110.40ms  cudaMalloc\n",
            "                   19.46%  86.891ms         3  28.964ms  14.784ms  56.654ms  cudaMemcpy\n",
            "                    0.58%  2.6029ms         3  867.64us  323.92us  1.1783ms  cudaFree\n",
            "                    0.07%  302.81us         1  302.81us  302.81us  302.81us  cudaLaunchKernel\n",
            "                    0.04%  183.09us       114  1.6060us     188ns  70.302us  cuDeviceGetAttribute\n",
            "                    0.00%  20.335us         1  20.335us  20.335us  20.335us  cudaGetLastError\n",
            "                    0.00%  12.405us         1  12.405us  12.405us  12.405us  cuDeviceGetName\n",
            "                    0.00%  6.9950us         1  6.9950us  6.9950us  6.9950us  cuDeviceGetPCIBusId\n",
            "                    0.00%  5.6860us         1  5.6860us  5.6860us  5.6860us  cuDeviceTotalMem\n",
            "                    0.00%  2.0710us         3     690ns     340ns  1.3870us  cuDeviceGetCount\n",
            "                    0.00%  1.0260us         2     513ns     297ns     729ns  cuDeviceGet\n",
            "                    0.00%     455ns         1     455ns     455ns     455ns  cuDeviceGetUuid\n",
            "                    0.00%     416ns         1     416ns     416ns     416ns  cuModuleGetLoadingMode\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For N = 1 << 10 (1024 x 1024): Thread coarsening is not worth it because the thread blocks are not being serialized by the hardware. Different thread blocks loading the same input tile is redundant, but the price we pay here is worth it. Our matrices are too small.\n",
        "\n",
        "For N = 1 << 12  (4096 x 4096): Thread coarsening is worth it because the blocks are being serialized by the hardware. This way, the coarsened thread block loads the input tiles of M once and reuse them for multiple output tiles."
      ],
      "metadata": {
        "id": "z7wgq0mDz7o0"
      }
    }
  ]
}